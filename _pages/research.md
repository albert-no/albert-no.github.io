---
title: "ISL - Research"
layout: gridlay
excerpt: "Research"
sitemap: false
permalink: /research/
---
<p>&nbsp;</p>
# Research

## Differential Privacy
<div class="row">
  <img src="{{ site.url }}{{ site.baseurl }}/images/respic/dp.png" class="center" width="50%" style="float: left" />
  Recent deep learning models, including advanced generative models, are susceptible to privacy breaches.
  Differential privacy is a robust statistical technique that protects individual data privacy within datasets while enabling precise aggregate data analysis.
  Our group is dedicated to integrating differential privacy into various deep learning frameworks,
  such as large language models and diffusion generative models, to enhance their privacy safeguards.
  <li> <a href="https://ealizadeh.com/blog/abc-of-differential-privacy/">img source</a></li>
</div>

## Diffusion Generative Models
<div class="row">
  <img src="{{ site.url }}{{ site.baseurl }}/images/respic/diffusion.png" class="center" width="50%" style="float: left" />
  Recent advancements in generative models utilizing diffusion processes have significantly impacted practical applications.
  These models are deeply rooted in the theoretical frameworks of stochastic differential equations, probability theory, and optimization.
  Our research group is committed to further exploring diffusion models and their mechanisms of sample generation,
  examining them through the perspectives of mathematics and information theory.
  <li> <a href="https://arxiv.org/pdf/2011.13456.pdf">img source</a></li>
</div>

## Deep Learning Theory
<div class="row">
  <img src="{{ site.url }}{{ site.baseurl }}/images/respic/wgan.png" class="center" width="50%" style="float: left" />
The tremendous recent progress of deep neural networks does not have sufficient theoretical grounds.
Our group focuses on theoretical understanding deep neural networks,
 including trainability guarantee, behavior of networks in infinite-wide(or depth) regime, and landscape of loss surfaces.
</div>

<p>&nbsp;</p>

## Neural Network Compression
<div class="row">
  <img src="{{ site.url }}{{ site.baseurl }}/images/respic/kd.png" class="img-responsive" width="50%" style="float: left" />
The recent success of NNs in various machine learning applications has come with their over-parameterization.
Deployment of such over-parameterized models on edge devices is challenging as these devices have limited storage,
 computation, and power resources.
Our group proposes light-weighted neural networks for various problems,
 including classification, object detection, and resmosaicing.
 We also studies theoretical perspective of neural network compression, including theoretical analysis of pruning.
</div>

<p>&nbsp;</p>

## DNA Storage
<div class="row">
  <img src="{{ site.url }}{{ site.baseurl }}/images/respic/dna.png" class="img-responsive" width="50%" style="float: left" />
DNA storage is one of the most promising next-generation storage system. 
It converts the data in quaternary format (A, C, G, and T), then synthesizes corresponding DNA strands to store data.
Our group works on error correcting codes for DNA synthesis and sequencing, image encoding for DNA storage,
and database structure for DNA storage.
</div>

<p>&nbsp;</p>

## Information Theory
<div class="row">
  <img src="{{ site.url }}{{ site.baseurl }}/images/respic/sr.png" class="img-responsive" width="50%" style="float: left" />
Information Theory provides a mathematical tools for practical engineering problems,
 including communications, estimations, and learning.
Our group focuses on connection between information theory and compression, and its application to light-weighted neural networks.
</div>
