<p>&nbsp;</p>
<h1 id="research">Research</h1>

<h2 id="deep-learning-theory">Deep Learning Theory</h2>
<div class="row">
  <img src="http://localhost:4000/images/respic/wgan.png" class="center" width="50%" style="float: left" />
The tremendous recent progress of deep neural networks does not have sufficient theoretical grounds.
Our group focuses on theoretical understanding deep neural networks,
 including trainability guarantee, behavior of networks in infinite-wide(or depth) regime, and landscape of loss surfaces.
</div>

<p>&nbsp;</p>

<h2 id="neural-network-compression">Neural Network Compression</h2>
<div class="row">
  <img src="http://localhost:4000/images/respic/kd.png" class="img-responsive" width="50%" style="float: left" />
The recent success of NNs in various machine learning applications has come with their over-parameterization.
Deployment of such over-parameterized models on edge devices is challenging as these devices have limited storage,
 computation, and power resources.
Our group proposes light-weighted neural networks for various problems,
 including classification, object detection, and resmosaicing.
 We also studies theoretical perspective of neural network compression, including theoretical analysis of pruning.
</div>

<p>&nbsp;</p>

<h2 id="dna-storage">DNA Storage</h2>
<div class="row">
  <img src="http://localhost:4000/images/respic/dna.png" class="img-responsive" width="50%" style="float: left" />
DNA storage is one of the most promising next-generation storage system. 
It converts the data in quaternary format (A, C, G, and T), then synthesizes corresponding DNA strands to store data.
Our group works on error correcting codes for DNA synthesis and sequencing, image encoding for DNA storage,
and database structure for DNA storage.
</div>

<p>&nbsp;</p>

<h2 id="information-theory">Information Theory</h2>
<div class="row">
  <img src="http://localhost:4000/images/respic/sr.png" class="img-responsive" width="50%" style="float: left" />
Information Theory provides a mathematical tools for practical engineering problems,
 including communications, estimations, and learning.
Our group focuses on connection between information theory and compression, and its application to light-weighted neural networks.
</div>
